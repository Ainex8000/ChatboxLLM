# Transformers origins are a paper called "Attention is all you need (2017)"
 
# Transformers are a type of neural network that uses attention
# Attention is a mechanism that allows the model to focus on certain parts
# of the input sequence when predicting a certain part of the output sequence
 
# This was by google and is called the transformer architecture
# its original purpose was to create a better
# translation of human languages
 
# To generate output, llms takes a vector of floats as input
# vectors are n-dimensional arrays with floating point values
 
# Note that this must download the first time you run it
# after that it will be cached.
 
# 'What color is the beautiful sky?'
# We need to convert this line into a vector of floats
# This is done by tokenization
 
# Tokenization is the process of converting a sequence of characters
# into a sequence of tokens
# A token is a unit of text in a document
 
# pip install transformers
 
# Import the tokenizer we will use
# We are using autotokenizer becuase it will automatically download the tokenizer
# that is associated with the model name we are using
# there are many different tokenizers classes
# that are associated with different models

# Import the tokenizer we will use
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig

# Clear the screen
print("\n"*100)
# Print out a title
print("\n\n\t *** Welcome to LLM Ranch ***")
# This is our hard coded input text
input_text = 'What is the capitol city of New Mexico?'
# Now override the hard coded input text with user input
input_text = input("\n\nPlease enter a prompt question > ")
# Print a line to make the output easier to read
print("\n")
print("-"*50)
print("The raw input text is:")
print("-"*50)
print(input_text)
print("\n")

# Our model name
# This is a huggingface model name.
# The syntax is namespace/model name
# There are different sizes of models
# The bigger the model the more accurate it is and
# the longer it takes to load and run
# The t5 base model is the second smallest t5 model
# The largest t5 model is the xxl model,
# The best model for our purposes is the t5 xl model
# The first time you load a model expect it to take a while esspecially the large ones, but it will be cached
# so it will be faster the next time you run it.

# No need to external download the model, it will be downloaded automatically
# Only have one uncommented at a time while evaluating models
model_name = 'google/flan-t5-small'
# model_name = 'google/flan-t5-base'
# model_name = 'google/flan-t5-xxl'
 
# Create the tokenizer based on the model name
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Tokenize the input text
input_tokens = tokenizer.tokenize(input_text)
# print the type of the tokens_input
# Print a line to make the output easier to read
print("-"*50)
print("The type of tokens_input is a" , type(input_tokens))
print("Here we Convert the words into token just to show the process, it is not needed and we will use a different method to do this later.")
print("The tokens generated by the tokenizer are:")
print("-"*50)
# Print the list of tokens that the tokenizer created
for each_token in input_tokens:
    print(each_token)
 
# Convert the tokens into token ids just to show the process, it is not needed
# and we will use a different method to do this later.
input_token_ids = tokenizer.convert_tokens_to_ids(input_tokens)

# We ccould use shorter code to do this, but would not learn as much.
# input_token_ids = tokenizer(input_text, return_tensors='pt')
# pt is pytorch which is the framework we are using under the hood
# This is a tensor which is a multi-dimensional array

# Print the Input Token IDs
print("\n")
print("-"*50)
print("Here we Convert the tokens into token ids just to show the process, it is not needed and we will use a different method to do this later.")
print("The input token ids are:")
print("-"*50)
for each_token_id in input_token_ids:
    print(each_token_id)
print("\n")
# Now print the token and the token id together side by side on each line.
print("-"*50)
print("Here we Convert the tokens into token ids just to show the process, it is not needed and we will use a different method to do this later.")
print("The tokens and token ids are:")
print("-"*50)

# zip() function returns a zip object, which is an iterator of tuples
# where the first item in each passed iterator is paired together,
# and then the second item in each passed iterator are paired together etc.
# If the passed iterators have different lengths, the iterator with the least
# items decides the length of the new iterator.
# An iterator is an object that contains a countable number of values.
# An iterator is an object that can be iterated upon.

for each_token, each_token_id in zip(input_tokens, input_token_ids):
    print(each_token, each_token_id)
print("\n")
 
# !!! The tensors below are the actual format we need to input to the model to get a response. !!!
# This is doing the number crunching and conversion for us.
# THIS IS A MAIN POINT OF THIS CODE
# Add a loop so the user can enter more prompts without having to run the program again.
is_alive = True
while is_alive:
 
    py_torch_input_tokens = tokenizer(input_text, return_tensors='pt')
    # pt is pytorch which is the framework we are using under the hood
    # This is a tensor which is a multi-dimensional array
    # print the type of the py_torch_input_tokens
    # Print a line to make the output easier to read
    print("-"*50)
    print("The type of py_torch_input_tokens is a" , type(py_torch_input_tokens))
    print("!!! The tensors below are the actual format we need to input to the model to get a response. !!!" )
    print("The output generated by the py_torch_input_tokens are:")
    print("-"*50)
    # Now print the input id's and the attention mask id together side by side on each line.
    for each_key in py_torch_input_tokens:
        print(each_key, py_torch_input_tokens[each_key])
    print("\n")
    # Now print the tokens and the attention mask id together side by side on each line.
    print("-"*50)
    print("Another view of the input_ids tensors / attention_mask tensors are:")
    print("-"*50)
    for each_token, each_attention_mask in zip(py_torch_input_tokens['input_ids'][0], py_torch_input_tokens['attention_mask'][0]):
        print(each_token, "/",each_attention_mask)
    print("\n")
 
    '''
    # The tokens are a list of strings that are produced by the tokenizer
    # for each word in the input text for efficient processing.
    # The tokenizer also adds special tokens to the list
    # that are used by the model to understand the input text
    # and to generate the output text
    # The special tokens are <s> and </s> which are used to mark 
    # the beginning and end of the text
    # and <pad> which is used to pad the input text to a fixed length
    # The tokenizer also adds a token called <unk> which is used to 
    # represent unknown tokens
    # that are not in the vocabulary of the tokenizer
    # The ? is converted to a token called <mask> which is used to
    # mask the input text, so that the model can predict the masked token.
    # for example, if the input text is 'What color is the beautiful sky?'
    # and the ? is converted to a token called <mask>
    # then the model will predict the masked token
    # which is the word 'beautiful' in this case because the model has
    # been trained to predict the masked token based on the context
    # of the input text
    '''
    '''
    # The tokenizer also adds a token called <pad> which is used to 
    # pad the input text to a fixed length
    # there are always
 
    # The tokens you pass into the model are called input tokens and the tokens
    # needed to be in that model when that model was pre trained.
 
    # The model doesn't understand the tokens, the model understands numbers
    # The tokenizer converts the tokens into numbers called token ids
    # The token ids are a list of integers that are produced by the tokenizer
 
    '''
    '''
    # This is not needed, but is a good way to see the process
    # The input embeddings are the input token ids
    # lets look under the hood.
    # The input embeddings are part of the model so we need to import the model
    # Don't forget to import the AutoModelForSeq2SeqLM
    # This automodel will choose the correct class to access the embeddings
    # based on the model name we are using
    '''
    model_embeddings = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    # Get the input embeddings
    input_embeddings = model_embeddings.get_input_embeddings()
    # Create a tensor to pass into the input_embeddings
    tensor_input_token_ids = py_torch_input_tokens['input_ids'][0]
    # See the actual vectors that came from our input string
    input_embeddings_vectors = input_embeddings(tensor_input_token_ids)
    # Print the input embeddings vectors
    print("-"*50)
    print("Although this is not something you would ever want to look at,")
    print("it is a good way to see the process.")
    print("The input embeddings vectors are:")
    print("-"*50)
    print(input_embeddings_vectors)
    print("\n")
    # Print the input embeddings vectors shape
    print("-"*50)
    print("Although this is not something you would ever want to look at,")
    print("it is a good way to see the process.")
    print("The input embeddings vectors shape is:")
    print("-"*50)
    print(input_embeddings_vectors.shape)
    print("\n")
    '''
    # For example if the result is torch.Size([ 8, 768])
    # then the input text was tokenized into 8 tokens
    # and each token was converted into a vector of 768 floats
    # This is 768 dimensional tokens for each token
    # This means the model in the input enbeddings 
    # has 768 neurons for each token
    '''
    print(''' For example if the result above is torch.Size([ 8, 768]) 
    then the input text was tokenized into 8 tokens
    and each token was converted into a vector of 768 floats
    This is 768 dimensional tokens for each token
    This means the model in the input enbeddings 
    has 768 neurons for each token
    Or 768 features for each token to determine the symantic
    meaning of the token. There is more to it, but this is a good start.
    ''')
    '''  
    # Now we will use the model to generate the output text from the model
    # The double ** is a python syntax that is used to pass a dictionary
    # as a set of keyword arguments
    # The model will generate the output text based on the input text
 
    '''
    # required to set a config for the model or you will get a warning
    # Here we set the max length of the output text
    # Don't forget to import the GenerationConfig
    # TODO: Play around witht he config, there are many more settings than we are using now!
    model_response_config = GenerationConfig(max_new_tokens=200)
    model_output_response_raw = model_embeddings.generate(**py_torch_input_tokens, generation_config=model_response_config)
    '''
    # now we need to decode the output text using the tokenizer
    # We will use batch because it can work with multiple inputs at the same time
    # This makes it faster and more efficient
    # Output tokens.
    # Special tokens are removed from the output text, we don't need them if we
    # are just looking at the output text to be readable by humans.
    '''
    model_output_response_decoded = tokenizer.batch_decode(model_output_response_raw, skip_special_tokens=True)
 
    # print model_output_response_decoded with the brackets and quotes
    print("-"*50)
    print("The model output response decoded is:")
    print("-"*50)
    print(model_output_response_decoded)
    print("\n")
 
    # print model_output_response_decoded without the brackets and quotes
    print("-"*50)
    print("The parsed and pretty model output response decoded is:")
    print("-"*50)
    print(*model_output_response_decoded)
    print("\n")
 
    # Now give the user the option to enter another prompt or exit
    # Ask the user if they want to enter another prompt
    # If the user enters 'y' then continue
    # If the user enters 'n' then exit
    # If the user enters anything else then ask again
    user_continue = input("Do you want to enter another prompt? (y/n) > ")
    if user_continue == 'y':
        # Clear the screen
        print("\n"*100)
        # Print out a title
        print("\n\n\t *** Welcome to LLM Ranch ***")
        # Now override the hard coded input text with user input
        input_text = input("\n\nPlease enter a prompt question > ")
        # Print a line to make the output easier to read
        print("\n")
        print("-"*50)
        print("The raw input text is:")
        print("-"*50)
        print(input_text)
        print("\n")
    else:
        # Set is_alive to False to exit the loop
        is_alive = False
        # Print a message to the user
        print("\n")
        print("-"*50)
        print("Thank you for using LLM Ranch.")
        print("Have a nice day!")
        print("-"*50)
        print("\n\n\n")
        